{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Training create_enhanced_model\n",
      "Training fold 1\n",
      "Epoch 1/2\n",
      "\u001b[1m 16/449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:35\u001b[0m 497ms/step - accuracy: 0.1761 - loss: 1.9654"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 384\u001b[0m\n\u001b[0;32m    381\u001b[0m model_funcs \u001b[38;5;241m=\u001b[39m [create_enhanced_model, create_cnn_model, create_resnet_model, create_inception_model, create_attention_model]\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Train models with cross-validation\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m best_model, scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_funcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Evaluate on the validation set\u001b[39;00m\n\u001b[0;32m    387\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mevaluate(validation_generator)\n",
      "Cell \u001b[1;32mIn[9], line 285\u001b[0m, in \u001b[0;36mtrain_with_cross_validation\u001b[1;34m(model_funcs, train_data, n_splits)\u001b[0m\n\u001b[0;32m    278\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    279\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[0;32m    280\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    281\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    282\u001b[0m )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased from 2 to 50\u001b[39;49;00m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This should be a separate validation set\u001b[39;49;00m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_fold_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    296\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(train_data)  \u001b[38;5;66;03m# This should be a separate validation set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Amine2001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Define categories for facial emotions\n",
    "categories = ['angry', 'fear', 'happy', 'neutral', 'sad', 'surprise', 'disgust']\n",
    "\n",
    "# Set up image parameters\n",
    "img_size = 48  # Size of input images\n",
    "batch_size = 64  # Number of images per batch\n",
    "train_path = 'train/'  # Path to training data\n",
    "test_path = 'test/'  # Path to test data\n",
    "\n",
    "def create_data_generators():\n",
    "    \"\"\"\n",
    "    Create and return data generators for training and validation.\n",
    "    \n",
    "    This function sets up data augmentation for training data to increase\n",
    "    model robustness and preprocessing for validation data.\n",
    "    \n",
    "    Returns:\n",
    "    - train_generator: ImageDataGenerator for training data\n",
    "    - validation_generator: ImageDataGenerator for validation data\n",
    "    \"\"\"\n",
    "    # Data augmentation for training data\n",
    "    datagen_train = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,  # Normalize pixel values\n",
    "        rotation_range=20,  # Randomly rotate images\n",
    "        width_shift_range=0.2,  # Randomly shift image horizontally\n",
    "        height_shift_range=0.2,  # Randomly shift image vertically\n",
    "        zoom_range=0.2,  # Randomly zoom image\n",
    "        horizontal_flip=True,  # Randomly flip images horizontally\n",
    "        brightness_range=[0.8, 1.2],  # Randomly adjust brightness\n",
    "        shear_range=0.1,  # Apply shearing transformations\n",
    "        fill_mode='nearest'  # Fill in newly created pixels\n",
    "    )\n",
    "\n",
    "    # Minimal augmentation for validation data\n",
    "    datagen_validation = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,  # Normalize pixel values\n",
    "        rotation_range=10,  # Slight rotation for validation\n",
    "        width_shift_range=0.1,  # Slight horizontal shift\n",
    "        height_shift_range=0.1,  # Slight vertical shift\n",
    "        zoom_range=0.1  # Slight zoom\n",
    "    )\n",
    "\n",
    "    # Create generator for training data\n",
    "    train_generator = datagen_train.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(img_size, img_size),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Create generator for validation data\n",
    "    validation_generator = datagen_validation.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(img_size, img_size),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Model Architectures\n",
    "\n",
    "def create_enhanced_model():\n",
    "    \"\"\"\n",
    "    Create and return an enhanced model using EfficientNetB0 as the base.\n",
    "    \n",
    "    This model uses transfer learning from EfficientNetB0 pre-trained on ImageNet.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNetB0 model\n",
    "    base_model = keras.applications.EfficientNetB0(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_shape=(img_size, img_size, 3)\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    # Build the model\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 1))\n",
    "    x = keras.layers.Conv2D(3, (1, 1))(inputs)  # Convert grayscale to RGB\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    outputs = keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='EnhancedModel')\n",
    "    return model\n",
    "\n",
    "def create_cnn_model():\n",
    "    \"\"\"\n",
    "    Create and return a simple CNN model.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Compiled Keras sequential model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(len(categories), activation='softmax')\n",
    "    ], name='CNNModel')\n",
    "    return model\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    \"\"\"\n",
    "    Create a residual block for the ResNet model.\n",
    "    \n",
    "    Args:\n",
    "    - x: Input tensor\n",
    "    - filters: Number of filters in the convolutional layer\n",
    "    - kernel_size: Size of the convolutional kernel\n",
    "    - stride: Stride for the convolutional layer\n",
    "    \n",
    "    Returns:\n",
    "    - x: Output tensor after applying the residual block\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "    x = keras.layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = keras.layers.Conv2D(filters, 1, strides=stride, padding='same')(shortcut)\n",
    "        shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    x = keras.layers.Add()([x, shortcut])\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model():\n",
    "    \"\"\"\n",
    "    Create and return a ResNet-like model.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Compiled Keras model with ResNet architecture\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 1))\n",
    "    x = keras.layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 128, stride=2)\n",
    "    x = residual_block(x, 256, stride=2)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs, name='ResNetModel')\n",
    "    return model\n",
    "\n",
    "def inception_module(x, filters):\n",
    "    \"\"\"\n",
    "    Create an Inception module for the Inception model.\n",
    "    \n",
    "    Args:\n",
    "    - x: Input tensor\n",
    "    - filters: Number of filters for each convolution branch\n",
    "    \n",
    "    Returns:\n",
    "    - x: Output tensor after applying the Inception module\n",
    "    \"\"\"\n",
    "    branch1x1 = keras.layers.Conv2D(filters, 1, activation='relu')(x)\n",
    "    \n",
    "    branch3x3 = keras.layers.Conv2D(filters, 1, activation='relu')(x)\n",
    "    branch3x3 = keras.layers.Conv2D(filters, 3, padding='same', activation='relu')(branch3x3)\n",
    "    \n",
    "    branch5x5 = keras.layers.Conv2D(filters, 1, activation='relu')(x)\n",
    "    branch5x5 = keras.layers.Conv2D(filters, 5, padding='same', activation='relu')(branch5x5)\n",
    "    \n",
    "    branch_pool = keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)\n",
    "    branch_pool = keras.layers.Conv2D(filters, 1, activation='relu')(branch_pool)\n",
    "    \n",
    "    return keras.layers.Concatenate()([branch1x1, branch3x3, branch5x5, branch_pool])\n",
    "\n",
    "def create_inception_model():\n",
    "    \"\"\"\n",
    "    Create and return an Inception-like model.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Compiled Keras model with Inception architecture\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 1))\n",
    "    x = keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = keras.layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    x = inception_module(x, 64)\n",
    "    x = inception_module(x, 120)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs, name='InceptionModel')\n",
    "    return model\n",
    "\n",
    "def attention_block(x, filters):\n",
    "    \"\"\"\n",
    "    Create an attention block for the Attention model.\n",
    "    \n",
    "    Args:\n",
    "    - x: Input tensor\n",
    "    - filters: Number of filters in the convolutional layer\n",
    "    \n",
    "    Returns:\n",
    "    - x: Output tensor after applying the attention mechanism\n",
    "    \"\"\"\n",
    "    g = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    g = keras.layers.Reshape((1, 1, filters))(g)\n",
    "    g = keras.layers.Conv2D(filters // 8, 1)(g)\n",
    "    g = keras.layers.Conv2D(filters, 1, activation='sigmoid')(g)\n",
    "    return keras.layers.Multiply()([x, g])\n",
    "\n",
    "def create_attention_model():\n",
    "    \"\"\"\n",
    "    Create and return an Attention-based model.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Compiled Keras model with Attention mechanism\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 1))\n",
    "    x = keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "    x = keras.layers.MaxPooling2D(2)(x)\n",
    "    x = keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "    x = attention_block(x, 64)\n",
    "    x = keras.layers.MaxPooling2D(2)(x)\n",
    "    x = keras.layers.Conv2D(128, 3, activation='relu')(x)\n",
    "    x = attention_block(x, 128)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs, name='AttentionModel')\n",
    "    return model\n",
    "\n",
    "# Training Function with Cross-Validation and Model Selection\n",
    "def train_with_cross_validation(model_funcs, train_data, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train multiple models using K-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "    - model_funcs: List of functions to create different models\n",
    "    - train_data: Training data generator\n",
    "    - n_splits: Number of splits for cross-validation\n",
    "    \n",
    "    Returns:\n",
    "    - best_model: The best performing model\n",
    "    - best_scores: List of accuracy scores for the best model\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_model = None\n",
    "    best_scores = []\n",
    "    best_avg_score = 0\n",
    "\n",
    "    for model_func in model_funcs:\n",
    "        print(f\"Training {model_func.__name__}\")\n",
    "        all_scores = []\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data.filenames)):\n",
    "            print(f\"Training fold {fold + 1}\")\n",
    "            \n",
    "            model = model_func()\n",
    "            \n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=optimizer, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            history = model.fit(\n",
    "                train_data,\n",
    "                epochs=2,  # Increased from 2 to 50\n",
    "                validation_data=train_data,  # This should be a separate validation set\n",
    "                callbacks=[\n",
    "                    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "                    keras.callbacks.ModelCheckpoint(f\"best_{model_func.__name__}_fold_{fold}.keras\", save_best_only=True)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Evaluate the model\n",
    "            scores = model.evaluate(train_data)  # This should be a separate validation set\n",
    "            all_scores.append(scores[1])  # Append accuracy\n",
    "        \n",
    "        avg_score = np.mean(all_scores)\n",
    "        print(f\"Average accuracy for {model_func.__name__}: {avg_score}\")\n",
    "        \n",
    "        if avg_score > best_avg_score:\n",
    "            best_avg_score = avg_score\n",
    "            best_model = model\n",
    "            best_scores = all_scores\n",
    "    \n",
    "    print(f\"Best model: {best_model.name} with average accuracy: {best_avg_score}\")\n",
    "    return best_model, best_scores\n",
    "\n",
    "# Grad-CAM Visualization\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM heatmap for the given image and model.\n",
    "    \n",
    "    Args:\n",
    "    - img_array: Input image as a numpy array\n",
    "    - model: Trained Keras model\n",
    "    - last_conv_layer_name: Name of the last convolutional layer in the model\n",
    "    - pred_index: Index of the predicted class (if None, uses the highest scoring class)\n",
    "    \n",
    "    Returns:\n",
    "    - heatmap: Grad-CAM heatmap as a numpy array\n",
    "    \"\"\"\n",
    "    grad_model = keras.models.Model(\n",
    "        [model.inputs], \n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def display_gradcam(img, heatmap, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Display the original image and its Grad-CAM heatmap overlay.\n",
    "    \n",
    "    Args:\n",
    "    - img: Original input image\n",
    "    - heatmap: Grad-CAM heatmap\n",
    "    - alpha: Transparency of the heatmap overlay\n",
    "    \"\"\"\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    jet = plt.cm.get_cmap(\"jet\")\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title(\"Grad-CAM\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create data generators\n",
    "    train_generator, validation_generator = create_data_generators()\n",
    "    \n",
    "    # List of model creation functions\n",
    "    model_funcs = [create_enhanced_model, create_cnn_model, create_resnet_model, create_inception_model, create_attention_model]\n",
    "    \n",
    "    # Train models with cross-validation\n",
    "    best_model, scores = train_with_cross_validation(model_funcs, train_generator)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    evaluation = best_model.evaluate(validation_generator)\n",
    "    print(f\"Test accuracy: {evaluation[1]*100:.2f}%\")\n",
    "    \n",
    "    # Test if the model works with a sample input\n",
    "    sample_image = next(iter(validation_generator))[0][0]\n",
    "    sample_image_array = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "    # Pass the sample image through the model (this builds the model if needed)\n",
    "    best_model.predict(sample_image_array)\n",
    "    \n",
    "    # Find the last convolutional layer\n",
    "    last_conv_layer = None\n",
    "    for layer in reversed(best_model.layers):\n",
    "        if isinstance(layer, keras.layers.Conv2D):\n",
    "            last_conv_layer = layer.name\n",
    "            break\n",
    "    \n",
    "    if last_conv_layer:\n",
    "        heatmap = make_gradcam_heatmap(sample_image_array, best_model, last_conv_layer)\n",
    "        display_gradcam(sample_image, heatmap)\n",
    "    else:\n",
    "        print(\"Could not find a convolutional layer for Grad-CAM visualization.\")\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    y_pred = best_model.predict(validation_generator)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = validation_generator.classes\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=categories))\n",
    "    \n",
    "    # Calculate and print additional metrics\n",
    "    f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "    \n",
    "    print(f\"\\nAdditional Metrics:\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n",
    "    print(f\"Precision (weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (weighted): {recall:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model.save(\"best_emotion_recognition_model.keras\")\n",
    "    print(\"\\nBest model saved as 'best_emotion_recognition_model.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
